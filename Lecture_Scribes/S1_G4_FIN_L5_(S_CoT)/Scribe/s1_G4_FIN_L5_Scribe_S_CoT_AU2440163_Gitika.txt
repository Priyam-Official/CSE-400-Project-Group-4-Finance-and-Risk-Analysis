\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{L5_scribe}
\author{Gitika Agarwal}
\date{February 2026}

\documentclass[12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{geometry}
\geometry{margin=1in}

\begin{document}

\section*{Lecture 5: Bayes’ Theorem, Random Variables, and Probability Mass Function}

\subsection*{Bayes’ Theorem -- Weighted Average of Conditional Probabilities}

Let $A$ and $B$ be events. We may express $A$ as
\[
A = AB \cup AB^c
\]

As $AB$ and $AB^c$ are mutually exclusive, by Axiom 3,
\[
\Pr(A) = \Pr(AB) + \Pr(AB^c)
\]
\[
= \Pr(A \mid B)\Pr(B) + \Pr(A \mid B^c)[1 - \Pr(B)]
\]

The probability of event $A$ is a weighted average of the conditional probabilities with weights given as the probability of the event on which it is conditioned has of occurring.

\subsection*{Bayes’ Theorem -- Learning by Example}

\textbf{Example 3.1 (Part 1/2):}

Accident-prone person: probability of accident $= 0.4$

Not accident-prone: probability of accident $= 0.2$

$30\%$ of population is accident prone

Let $A_1 =$ event that policyholder has an accident within a year

Let $A =$ event that policyholder is accident prone

\[
\Pr(A_1) = \Pr(A_1 \mid A)\Pr(A) + \Pr(A_1 \mid A^c)\Pr(A^c)
\]
\[
= (0.4)(0.3) + (0.2)(0.7) = 0.26
\]

\textbf{Example 3.1 (Part 2/2):}

Given that a policyholder has an accident, find the probability that he or she is accident prone.

\[
\Pr(A \mid A_1) = \frac{\Pr(A_1 A)}{\Pr(A_1)}
\]
\[
= \frac{\Pr(A)\Pr(A_1 \mid A)}{\Pr(A_1)}
\]
\[
= \frac{(0.3)(0.4)}{0.26} = \frac{6}{13}
\]

\subsection*{Bayes’ Theorem -- Formal Introduction}

\textbf{Law of Total Probability (Formula 3.4)}

Using
\[
\Pr(AB_i) = \Pr(B_i \mid A)\Pr(A)
\]

\textbf{Bayes Formula (Proposition 3.1)}

$\Pr(B_i)$ is the apriori probability

$\Pr(B_i \mid A)$ is the posteriori probability

\subsection*{Bayes Formula -- Learning by Example}

\textbf{Example 3.2:}

Three cards: RR, BB, RB

One card selected at random and placed on ground

Upper side is red

Let $R =$ event that upper side is red

Desired probability: $\Pr(RB \mid R)$

\[
\Pr(RB \mid R) = \frac{\Pr(RB \cap R)}{\Pr(R)}
\]
\[
= \frac{\Pr(R \mid RB)\Pr(RB)}{\Pr(R \mid RR)\Pr(RR) + \Pr(R \mid RB)\Pr(RB) + \Pr(R \mid BB)\Pr(BB)}
\]
\[
= \frac{\left(\frac{1}{2} \cdot \frac{1}{3}\right)}{\left(1 \cdot \frac{1}{3}\right) + \left(\frac{1}{2} \cdot \frac{1}{3}\right) + \left(0 \cdot \frac{1}{3}\right)} = \frac{1}{3}
\]

\subsection*{Random Variables -- Motivation and Concept}

A random variable is a real-valued function defined on the sample space.

Values are determined by the outcomes of an experiment.

Probabilities are assigned to possible values of random variables.

Examples include sum of dice and number of heads in coin tosses.

The distribution of a random variable can be visualized as a bar diagram.

The x-axis represents values of the random variable.

The height of the bar at value $a$ is $\Pr[X = a]$.

\subsection*{Random Variables -- Example}

Tossing 3 fair coins.

Let $Y =$ number of heads.

Possible values: $0, 1, 2, 3$

\[
P\{Y = 0\} = \frac{1}{8}
\]
\[
P\{Y = 1\} = \frac{3}{8}
\]
\[
P\{Y = 2\} = \frac{3}{8}
\]
\[
P\{Y = 3\} = \frac{1}{8}
\]

Since $Y$ must take one of these values, the probabilities sum to 1.

\subsection*{Probability Mass Function -- Concept}

A random variable that can take at most a countable number of possible values is discrete.

Let $X$ be a discrete random variable with range
\[
R_X = x_1, x_2, x_3, \ldots
\]

The function
\[
p(x) = \Pr\{X = x\}
\]
is called the Probability Mass Function (PMF) of $X$.

Since $X$ must take one of the values $x_k$,
\[
\sum p(x_k) = 1
\]

\subsection*{Probability Mass Function -- Example}

The PMF is given by
\[
p(i) = c \frac{\lambda^i}{i!}, \quad i = 0, 1, 2, \ldots
\]

Find $P\{X = 0\}$ and $P\{X > 2\}$



\end{document}
