\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{geometry}
\usepackage{enumitem}
\geometry{margin=1in}

\title{\textbf{CSE400: Fundamentals of Probability in Computing}\\
Lecture 5: Bayes' Theorem, Random Variables, and Probability Mass Function}
\author{\\ \textbf{Name: Masoom Choksi} \\ \textbf{AUID: AU2440263}}
\date{}

\begin{document}
\maketitle

\section*{Bayes' Theorem}

\subsection*{Weighted Average of Conditional Probabilities}

Let $A$ and $B$ be events. We may express the event $A$ as
\[
A = AB \cup AB^c,
\]
since in order for an outcome to be in $A$, it must either be in both $A$ and $B$, or be in $A$ but not in $B$.

Since $AB$ and $AB^c$ are mutually exclusive, by Axiom 3 of probability,
\begin{align*}
\Pr(A) &= \Pr(AB) + \Pr(AB^c) \\
&= \Pr(A \mid B)\Pr(B) + \Pr(A \mid B^c)\Pr(B^c) \\
&= \Pr(A \mid B)\Pr(B) + \Pr(A \mid B^c)\left[1 - \Pr(B)\right].
\end{align*}

Thus, the probability of event $A$ is a weighted average of the conditional probabilities, with weights given by the probabilities of the events on which it is conditioned.

\subsection*{Learning by Example}

\paragraph{Example 3.1 (Part 1)}

An insurance company classifies people into two categories:
\begin{itemize}
\item Accident-prone
\item Not accident-prone
\end{itemize}

The probability that an accident-prone person has an accident within a 1-year period is $0.4$, while this probability is $0.2$ for a person who is not accident-prone. It is assumed that $30\%$ of the population is accident-prone.

\textbf{Question:} What is the probability that a new policyholder will have an accident within a year?

\textbf{Solution:}

Let
\begin{itemize}
\item $A_1$ denote the event that the policyholder has an accident within a year,
\item $A$ denote the event that the policyholder is accident-prone.
\end{itemize}

Then,
\begin{align*}
\Pr(A_1) &= \Pr(A_1 \mid A)\Pr(A) + \Pr(A_1 \mid A^c)\Pr(A^c) \\
&= (0.4)(0.3) + (0.2)(0.7) \\
&= 0.12 + 0.14 \\
&= 0.26.
\end{align*}

\paragraph{Example 3.1 (Part 2)}

Suppose that a new policyholder has an accident within a year. What is the probability that the policyholder is accident-prone?

\textbf{Solution:}

We compute
\[
\Pr(A \mid A_1) = \frac{\Pr(A \cap A_1)}{\Pr(A_1)}.
\]
Using $\Pr(A \cap A_1) = \Pr(A)\Pr(A_1 \mid A)$,
\begin{align*}
\Pr(A \mid A_1) &= \frac{\Pr(A)\Pr(A_1 \mid A)}{\Pr(A_1)} \\
&= \frac{(0.3)(0.4)}{0.26} \\
&= \frac{6}{13}.
\end{align*}

\subsection*{Formal Introduction}

\subsubsection*{Law of Total Probability}

Let $B_1, B_2, \dots, B_n$ be mutually exclusive and exhaustive events such that
\[
\bigcup_{i=1}^n B_i = S.
\]
Then for any event $A$,
\[
\Pr(A) = \sum_{i=1}^n \Pr(A \cap B_i).
\]
This is known as the \textbf{Law of Total Probability}.

\subsubsection*{Bayes' Formula}

Using
\[
\Pr(A \cap B_i) = \Pr(B_i \mid A)\Pr(A),
\]
we obtain
\[
\Pr(B_i \mid A) = \frac{\Pr(A \mid B_i)\Pr(B_i)}{\sum_{j=1}^n \Pr(A \mid B_j)\Pr(B_j)}.
\]

This is known as \textbf{Bayes' Formula} (Proposition 3.1), where:
\begin{itemize}
\item $\Pr(B_i)$ is the \textit{a priori} probability,
\item $\Pr(B_i \mid A)$ is the \textit{a posteriori} probability after observing event $A$.
\end{itemize}

\subsection*{Example 3.2}

Three cards are identical in form:
\begin{itemize}
\item One card has both sides red (RR),
\item One card has both sides black (BB),
\item One card has one red and one black side (RB).
\end{itemize}

One card is randomly selected and placed on the ground. If the upper side is red, what is the probability that the other side is black?

\textbf{Solution:}

Let
\begin{itemize}
\item $RR, RB, BB$ denote the events that the selected card is all-red, red-black, or all-black respectively,
\item $R$ denote the event that the upturned side is red.
\end{itemize}

We compute
\[
\Pr(RB \mid R) = \frac{\Pr(R \mid RB)\Pr(RB)}{\Pr(R)}.
\]

Now,
\[
\Pr(R) = \Pr(R \mid RR)\Pr(RR) + \Pr(R \mid RB)\Pr(RB) + \Pr(R \mid BB)\Pr(BB).
\]

Substituting values,
\begin{align*}
\Pr(RB \mid R) &= \frac{(1/2)(1/3)}{(1)(1/3) + (1/2)(1/3) + (0)(1/3)} \\
&= \frac{1/6}{1/2} \\
&= \frac{1}{3}.
\end{align*}

\section*{Random Variables}

\subsection*{Motivation and Concept}

In many experiments, we are interested in a function of the outcome rather than the outcome itself.

\begin{itemize}
\item In tossing two dice, the sum is often of interest.
\item In flipping coins, the number of heads may be of interest.
\end{itemize}

A \textbf{random variable} is a real-valued function defined on the sample space. Its value is determined by the outcome of an experiment, and probabilities are assigned to its possible values.

\subsection*{Example}

Suppose three fair coins are tossed. Let $Y$ denote the number of heads observed.

Then $Y$ takes values $\{0,1,2,3\}$ with probabilities:
\begin{align*}
\Pr(Y=0) &= \Pr(t,t,t) = \frac{1}{8}, \\
\Pr(Y=1) &= \Pr(t,t,h),(t,h,t),(h,t,t) = \frac{3}{8}, \\
\Pr(Y=2) &= \Pr(t,h,h),(h,t,h),(h,h,t) = \frac{3}{8}, \\
\Pr(Y=3) &= \Pr(h,h,h) = \frac{1}{8}.
\end{align*}

Since $Y$ must take one of these values,
\[
\sum_{k=0}^{3} \Pr(Y=k) = 1.
\]

\section*{Probability Mass Function}

\subsection*{Concept}

A random variable that takes at most a countable number of values is called \textbf{discrete}.

Let $X$ be a discrete random variable with range
\[
R_X = \{x_1, x_2, x_3, \dots\}.
\]

The function
\[
p(x_k) = \Pr(X = x_k)
\]
is called the \textbf{Probability Mass Function (PMF)} of $X$.

Since $X$ must take one of the values in $R_X$,
\[
\sum_k p(x_k) = 1.
\]

\subsection*{Example}

The PMF of a random variable $X$ is given by
\[
p(i) = c \lambda^i, \quad i = 0,1,2,\dots,
\]
where $\lambda > 0$.

Since probabilities must sum to 1,
\[
\sum_{i=0}^{\infty} c \lambda^i = 1.
\]

This gives
\[
c \sum_{i=0}^{\infty} \lambda^i = c \frac{1}{1-\lambda} = 1,
\]
hence
\[
c = 1-\lambda.
\]

Therefore,
\[
\Pr(X=0) = p(0) = 1-\lambda,
\]
and
\begin{align*}
\Pr(X>2) &= 1 - \Pr(X \le 2) \\
&= 1 - \sum_{i=0}^{2} (1-\lambda)\lambda^i.
\end{align*}

\end{document}
